{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Input,Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dense,Rescaling, MultiHeadAttention,BatchNormalization, Reshape\n",
    "from tensorflow.keras import activations\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten,Input, BatchNormalization, GlobalMaxPool1D,LSTM\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy, categorical_crossentropy\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn import preprocessing\n",
    "import scipy.spatial as sp\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import * \n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import skew, kurtosis, iqr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from numpy.linalg import norm\n",
    "from scipy import interpolate\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import librosa.display\n",
    "import gc\n",
    "from ssqueezepy import cwt\n",
    "from ssqueezepy.visuals import plot, imshow\n",
    "import cv2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Input Variables (Classes): \n",
      "['air' 'art' 'government' 'health']\n",
      "---------------------\n",
      "size of master_dataset_x:  24\n",
      "dimensions of features in master_dataset_x:  (675, 7)\n",
      "minimum length of sequence:  192\n",
      "maximum length of sequence:  2560\n",
      "(24, 700, 7)\n",
      "Unique Labels:  ['class1' 'class2' 'class3']\n",
      "data_X shape:  (12, 700, 14)\n",
      "data_Y shape:  (12,)\n",
      "one_hot_encoded shape:  (12, 3)\n",
      " class1  ----------------------------->  [1. 0. 0.]\n",
      " class2  ----------------------------->  [0. 1. 0.]\n",
      " class3  ----------------------------->  [0. 0. 1.]\n",
      "Progress:  11 / 12\r"
     ]
    }
   ],
   "source": [
    "#get list of all file names in directory \"data\"\n",
    "file_names = os.listdir(r\"D:/Research_work/Trace_Analysis/data5\")\n",
    "\n",
    "#get the part of names in file_names before '-'\n",
    "input_variables = [name.split('-')[0] for name in file_names]\n",
    "\n",
    "column_names =['ignore', 'command (read/write)', 'physical address', 'size','gem5 flags', 'timestamp','program_counter']\n",
    "#find unique elements in input_variables\n",
    "unique_input_variables = np.unique(input_variables)\n",
    "print(\"All Input Variables (Classes): \")\n",
    "print(unique_input_variables)\n",
    "print(\"---------------------\")\n",
    "\n",
    "\n",
    "master_dataset_x=[]\n",
    "master_dataset_y=[]\n",
    "\n",
    "\n",
    "knowledge=[]\n",
    "knowledge=[]\n",
    "\n",
    "for names in file_names:\n",
    "    #check if names contain string 'icachetrace'\n",
    "    if 'dcachetrace' in names:\n",
    "        number_in_txt=re.findall(r'\\d+', names)\n",
    "        number_in_txt=number_in_txt[0]\n",
    "        input_variable=names.split('-')[0]\n",
    "        df = pd.read_csv(\"D:/Research_work/Trace_Analysis/data5/\"+ names, sep=',', header=None)\n",
    "        df.columns = column_names\n",
    "\n",
    "        #find unique elements in column 'command (read/write)'\n",
    "        unique_commands=df['command (read/write)'].unique()\n",
    "\n",
    "        #create dictionary of unique_commands that integer encode the column 'command (read/write)'\n",
    "        dict_commands = {unique_commands[i]:i for i in range(len(unique_commands))}\n",
    "\n",
    "        #integer encode 'comand (read/write)' column\n",
    "        df['command (read/write)'] = df['command (read/write)'].map(dict_commands)\n",
    "\n",
    "        # convert df to np arrays\n",
    "        df = df.to_numpy()\n",
    "        possible_length=len(df)\n",
    "        master_dataset_x.append(df)\n",
    "        master_dataset_y.append(input_variable)\n",
    "        knowledge.append(input_variable+number_in_txt+'D')\n",
    "\n",
    "    if 'icachetrace' in names:\n",
    "        number_in_txt=re.findall(r'\\d+', names)\n",
    "        number_in_txt=number_in_txt[0]\n",
    "        input_variable=names.split('-')[0]\n",
    "        df = pd.read_csv(\"D:/Research_work/Trace_Analysis/data5/\"+ names, sep=',', header=None)\n",
    "        df.columns = column_names\n",
    "\n",
    "        #find unique elements in column 'command (read/write)'\n",
    "        unique_commands=df['command (read/write)'].unique()\n",
    "\n",
    "        #create dictionary of unique_commands that integer encode the column 'command (read/write)'\n",
    "        dict_commands = {unique_commands[i]:i for i in range(len(unique_commands))}\n",
    "\n",
    "        #integer encode 'comand (read/write)' column\n",
    "        df['command (read/write)'] = df['command (read/write)'].map(dict_commands)\n",
    "\n",
    "        # convert df to np arrays\n",
    "        df = df.to_numpy()\n",
    "        possible_length=len(df)\n",
    "        master_dataset_x.append(df)\n",
    "        master_dataset_y.append(input_variable)\n",
    "        knowledge.append(input_variable+number_in_txt+'I')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('size of master_dataset_x: ',len(master_dataset_x))\n",
    "print('dimensions of features in master_dataset_x: ',master_dataset_x[0].shape)\n",
    "\n",
    "\n",
    "#getting inhomogenous lengths\n",
    "len_seq=[]\n",
    "for i in range(len(master_dataset_x)):\n",
    "    len_seq.append(master_dataset_x[i].shape[0])\n",
    "\n",
    "min_len_seq=min(len_seq)\n",
    "print('minimum length of sequence: ',min_len_seq)\n",
    "max_len_seq=max(len_seq)\n",
    "print('maximum length of sequence: ',max_len_seq)\n",
    "\n",
    "\n",
    "\n",
    "# #padding zeros\n",
    "threshold=max_len_seq\n",
    "for i in range(len(master_dataset_x)):\n",
    "    #pad zeros if len of sequence is less than threshold\n",
    "    if master_dataset_x[i].shape[0]<threshold:\n",
    "        master_dataset_x[i]=np.pad(master_dataset_x[i],((0,threshold-master_dataset_x[i].shape[0]),(0,0)),'constant')\n",
    "\n",
    "\n",
    "#cropping data\n",
    "threshold=700\n",
    "for i in range(len(master_dataset_x)):\n",
    "    master_dataset_x[i]=master_dataset_x[i][:threshold,:]\n",
    "\n",
    "\n",
    "\n",
    "data_X_list=master_dataset_x\n",
    "data_Y_list=master_dataset_y\n",
    "\n",
    "\n",
    "#convert to np array\n",
    "data_X=np.array(data_X_list)\n",
    "data_Y=np.array(data_Y_list)\n",
    "\n",
    "\n",
    "\n",
    "print(data_X.shape)\n",
    "\n",
    "#horizonatlly stacking enteries of data_X corresponding to d and i in 'knowledge'\n",
    "data_X_new=[]\n",
    "data_Y_new=[]\n",
    "count=0\n",
    "for i in range(len(knowledge)):\n",
    "    k_name=knowledge[i]\n",
    "    if 'D' in k_name:\n",
    "        considered_name_d=k_name[:k_name.index('D')]\n",
    "        for j in range(len(knowledge)):\n",
    "            k_name2=knowledge[j]\n",
    "            if 'I' in k_name2:\n",
    "                considered_name_i=k_name2[:k_name2.index('I')]\n",
    "                if considered_name_d==considered_name_i:\n",
    "                    #get number in considered_name_d\n",
    "                    number_in_txt=re.findall(r'\\d+', considered_name_d)\n",
    "                    number_in_txt=number_in_txt[0]\n",
    "                    #get string before number in considered_name_d\n",
    "                    probable_class=considered_name_d[:considered_name_d.index(number_in_txt)]\n",
    "                    data_Y_new.append(probable_class)\n",
    "                    data_X_new.append(np.hstack((data_X[i],data_X[j])))\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_X_new=np.array(data_X_new)   \n",
    "data_X=data_X_new.copy()  \n",
    "data_Y=np.array(data_Y_new)\n",
    "\n",
    "\n",
    "#shuffle\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "data_X, data_Y = unison_shuffled_copies(data_X, data_Y)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Omega_Y_Out=data_Y.copy()\n",
    "\n",
    "# correcting labels\n",
    "for i in range(len(data_Y)):\n",
    "    subject=data_Y[i]\n",
    "    #check if subject has 10 characters\n",
    "    if len(subject)<=4:\n",
    "        data_Y[i]='class1'\n",
    "    if len(subject)<=6 and len(subject)>4:\n",
    "        data_Y[i]='class2'\n",
    "    if len(subject)<=10 and len(subject)>6:\n",
    "        data_Y[i]='class3'\n",
    "\n",
    "Omega_Y_In=data_Y.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#slicing some variables\n",
    "vars_2_keep=[i for i in range(14)]\n",
    "data_X=data_X[:,:,vars_2_keep]\n",
    "num_vars=len(vars_2_keep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #####-------smoothing-----##########\n",
    "for i in range(len(data_X)):\n",
    "    for j in range(data_X.shape[2]):\n",
    "        sig=data_X[i,:,j]\n",
    "        check_sig=sig.copy()\n",
    "        smooth_sig=gaussian_filter(sig, sigma=2)\n",
    "        data_X[i,:,j]=smooth_sig\n",
    "\n",
    "# ################################\n",
    "\n",
    "\n",
    "#partition data for now--again\n",
    "part=len(data_X)\n",
    "data_X=data_X[:part]\n",
    "data_Y=data_Y[:part]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Unique Labels: \",np.unique(data_Y))\n",
    "print(\"data_X shape: \", data_X.shape)\n",
    "print(\"data_Y shape: \", data_Y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#one hot encode data_Y\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(data_Y)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "num_classes=int(np.max(integer_encoded)+1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded).astype(float)\n",
    "\n",
    "print(\"one_hot_encoded shape: \", onehot_encoded.shape)\n",
    "mapping = dict(zip(label_encoder.classes_, onehot_encoder.transform(label_encoder.transform(label_encoder.classes_).reshape(num_classes, 1))))\n",
    "for key,value in mapping.items():\n",
    "    print(\"\",key,\" -----------------------------> \",value)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#PCA thingy from my COVID paper\n",
    "\n",
    "data_X_copy=data_X.copy()\n",
    "# -----------------sklearn minmax scaler\n",
    "for i in range(data_X.shape[2]):\n",
    "    scaler = StandardScaler()\n",
    "    stranded_data=data_X_copy[:,:,i]\n",
    "    standardized_stranded_data = scaler.fit_transform(stranded_data)\n",
    "    data_X[:,:,i]=standardized_stranded_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------sine transformation\n",
    "amp1=1.0\n",
    "amp2=0.01\n",
    "for i in range(len(data_X)):\n",
    "    for j in range(data_X.shape[2]):\n",
    "        if np.max(data_X[i,:,j])!=0:\n",
    "            data_X[i,:,j]=data_X[i,:,j]+amp1*np.sin(2*np.pi*data_X[i,:,j]/np.max(data_X[i,:,j]))+amp2*np.random.uniform(low=0.5, high=1.3, size=(data_X.shape[1],))\n",
    "        else:\n",
    "            data_X[i,:,j]=data_X[i,:,j]+amp1*np.sin(2*np.pi*data_X[i,:,j])+amp2*np.random.uniform(low=0.5, high=1.3, size=(data_X.shape[1],))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# wavelet spectrogram generation\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "IMG_SIZE=256\n",
    "\n",
    "\n",
    "for i in range(len(data_X)):\n",
    "    Im=[]\n",
    "    for j in range(data_X.shape[2]):\n",
    "        sig=data_X[i,:,j]\n",
    "        fig, ax = plt.subplots()\n",
    "        Wx, scales = cwt(sig, 'morlet')\n",
    "        imshow(Wx, abs=1)\n",
    "        fig.savefig('images_combined\\im'+str(i)+'_'+str(j)+'.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    #print progress on same line\n",
    "    print(\"Progress: \", i, \"/\", len(data_X), end=\"\\r\")\n",
    "\n",
    "\n",
    "np.save('data_X_combined.npy',data_X)\n",
    "np.save('data_Y_combined.npy',data_Y)\n",
    "np.save('onehot_encoded_combined.npy',onehot_encoded)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f014faad5b0bcfd18fa6746ff4cfcb8891c7c51a712e7498e1fbee7eccf19bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
